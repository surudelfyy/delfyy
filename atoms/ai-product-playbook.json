[
  {
    "id": "ai-product-playbook-heuristic-001",
    "source": "The AI Product Playbook",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "Start with the user problem, not the technology. Resist the urge to use AI simply because it is trendy—rigorously evaluate if a simpler rule-based solution creates equal value before committing to ML complexity.",
    "rationale": "AI projects require vast datasets, iterative training, and ongoing maintenance. If a simple heuristic solves 80% of the problem with 20% of the effort, prioritize that. ML is justified only when data-driven adaptability is genuinely required.",
    "lens": ["Customer", "Feasibility"],
    "level": "Product",
    "dimension": "Scope",
    "applies_when": ["technology:shiny", "problem:unclear"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-heuristic-002",
    "source": "The AI Product Playbook",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "AI models are probabilistic, not deterministic. Design product experiences that gracefully handle uncertainty—use language like 'We think you might like...' rather than 'You will like...' and build fallback mechanisms for incorrect predictions.",
    "rationale": "Traditional software gives absolute answers; AI gives confidence scores. Treating errors as bugs rather than inherent system behavior leads to frustrated users. UX must communicate uncertainty and keep users in control.",
    "lens": ["Customer"],
    "level": "Product",
    "dimension": "Value",
    "applies_when": ["ai:predictions", "UX:deterministic"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-heuristic-003",
    "source": "The AI Product Playbook",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "The Precision-Recall trade-off is a business decision, not a technical one. High Precision minimizes false alarms; high Recall minimizes missed cases. You must decide which error is more costly.",
    "rationale": "You cannot maximize both simultaneously. The choice determines who bears the cost of errors—users frustrated by false positives or users harmed by false negatives. This strategic decision belongs to the PM, not the data scientist.",
    "lens": ["Business", "Customer"],
    "level": "Product",
    "dimension": "Value",
    "applies_when": ["model:classification", "errors:asymmetric"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-heuristic-004",
    "source": "The AI Product Playbook",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "Your model is only as good as your data. Invest heavily in data quality, relevance, and governance from day one. Data quantity does not guarantee success—relevance and cleanliness matter more than volume.",
    "rationale": "Garbage in, garbage out. Many impactful projects begin with small, high-quality datasets. Your strategy must include data acquisition, rights verification, and quality control as foundational elements, not afterthoughts.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["data:prioritized-second", "quality:assumed"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-heuristic-005",
    "source": "The AI Product Playbook",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "Models are depreciating assets. Plan for regular retraining with fresh data. Data Drift and Concept Drift degrade performance over time. AI is a continuous lifecycle, not a project with a finish line.",
    "rationale": "Real-world data changes constantly. A model trained on last year's user behavior may fail on this year's users. Include maintenance and retraining costs in ROI calculations—AI is not 'build once, run forever.'",
    "lens": ["Feasibility", "Business"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["model:deployed", "monitoring:absent"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-heuristic-006",
    "source": "The AI Product Playbook",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "You cannot measure improvement without a baseline. Measure the performance of the current non-AI solution (or manual process) before development begins. Without a 'before' picture, you cannot prove the investment was justified.",
    "rationale": "ROI requires comparison. If no data exists, run a manual process or 'Wizard of Oz' experiment to generate baseline data. This discipline separates successful AI projects from expensive experiments.",
    "lens": ["Business"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["baseline:missing", "ROI:needed"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-heuristic-007",
    "source": "The AI Product Playbook",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "Human-in-the-Loop is a strategic design choice, not a failure of automation. Use HITL for data labeling, reviewing low-confidence predictions, or handling edge cases. Human corrections improve the model.",
    "rationale": "HITL improves reliability, mitigates bias, and builds user trust. For high-stakes decisions, an AI should never have final say without human review. Human corrections become training data for continuous improvement.",
    "lens": ["Customer", "Feasibility"],
    "level": "Product",
    "dimension": "Scope",
    "applies_when": ["decisions:high-stakes", "confidence:low"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-heuristic-008",
    "source": "The AI Product Playbook",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "LLMs are 'stochastic parrots'—they predict probable language patterns, not facts. Implement rigorous guardrails and evaluation processes to mitigate hallucinations.",
    "rationale": "Generative AI creates convincing but potentially false content. Without guardrails, users may act on misinformation. Evaluation processes must verify accuracy and safety before deployment.",
    "lens": ["Customer", "Feasibility"],
    "level": "Product",
    "dimension": "Scope",
    "applies_when": ["genai:deployed", "verification:missing"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-heuristic-009",
    "source": "The AI Product Playbook",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "Translate technical model metrics into business value explicitly. 'Improving recall by 5%' must be framed as business outcomes. A model with 95% accuracy is worthless if it doesn't move a business KPI.",
    "rationale": "Stakeholders care about revenue, churn, and efficiency. The PM must bridge technical performance and business impact to justify investment and demonstrate ROI.",
    "lens": ["Business"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["metrics:technical-only", "stakeholders:business"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-failuremode-001",
    "source": "The AI Product Playbook",
    "type": "FailureMode",
    "purpose": "Warn",
    "claim": "Overfitting: High performance on training data combined with low performance on testing data indicates memorization rather than learning. Never evaluate success on training data performance.",
    "rationale": "A model that memorizes specific examples fails on new data. Split data into Training, Validation, and Testing sets. The Testing set is the only true measure of generalization.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["evaluation:training-only", "generalization:untested"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-failuremode-002",
    "source": "The AI Product Playbook",
    "type": "FailureMode",
    "purpose": "Warn",
    "claim": "Bias in AI is systemic risk leading to legal penalties, reputational damage, and societal harm. Sources include Sample Bias, Prejudice Bias, and Proxy Features.",
    "rationale": "Anti-discrimination laws apply to AI. Training on biased data creates discriminatory systems. Fairness must be a core requirement from day one, not a post-launch fix.",
    "lens": ["Customer", "Business"],
    "level": "Product",
    "dimension": "Scope",
    "applies_when": ["data:biased", "ethics:deferred"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-failuremode-003",
    "source": "The AI Product Playbook",
    "type": "FailureMode",
    "purpose": "Warn",
    "claim": "Defining the wrong reward function in Reinforcement Learning causes the agent to optimize ruthlessly for the wrong outcome, potentially damaging user experience.",
    "rationale": "RL agents maximize whatever reward you define. Misaligned rewards lead to harmful behavior. The PM must design reward functions aligned with true user and business goals.",
    "lens": ["Customer", "Business"],
    "level": "Product",
    "dimension": "Value",
    "applies_when": ["RL:deployed", "incentives:misaligned"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-failuremode-004",
    "source": "The AI Product Playbook",
    "type": "FailureMode",
    "purpose": "Warn",
    "claim": "A vague problem definition leads to wasted engineering cycles and models that optimize for the wrong metric. The AI lifecycle begins with 'Why'—define user problem, business goal, and measurable objectives before touching a model.",
    "rationale": "Without clear problem definition, teams build technically impressive solutions that don't move business KPIs or solve real user problems. Objectives must be locked in before data collection begins.",
    "lens": ["Business"],
    "level": "Product",
    "dimension": "Scope",
    "applies_when": ["problem:vague", "development:started"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-pattern-001",
    "source": "The AI Product Playbook",
    "type": "Pattern",
    "purpose": "Evaluate",
    "claim": "The 7-Phase AI Lifecycle: 1) Problem Definition → 2) Data Collection/Exploration → 3) Preprocessing/Feature Engineering → 4) Model Selection/Training → 5) Evaluation/Tuning → 6) Deployment/Monitoring → 7) Retraining/Maintenance.",
    "rationale": "AI products are living lifecycles, not static projects. Each phase has PM responsibilities—from defining objectives to monitoring drift. Skipping phases undermines the investment.",
    "lens": ["Business", "Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["ai:building"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-pattern-002",
    "source": "The AI Product Playbook",
    "type": "Pattern",
    "purpose": "Evaluate",
    "claim": "Three AI PM Archetypes: AI-Experiences PM, AI-Builder PM, AI-Enhanced PM. Different roles require different skills.",
    "rationale": "Different roles require different skills. AI-Experiences PMs need UX and experimentation expertise. AI-Builder PMs need MLOps fluency. AI-Enhanced PMs need prompt engineering.",
    "lens": ["Business"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["career:planning", "role:defining"],
    "strength": "Medium",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-pattern-003",
    "source": "The AI Product Playbook",
    "type": "Pattern",
    "purpose": "Evaluate",
    "claim": "Map AI capabilities to problem types: Classification, Clustering, Regression/Prediction, Generation, Anomaly Detection. Match capability to problem.",
    "rationale": "Not all AI is the same. Wrong capability-problem match wastes resources. Choose capability based on the problem you need to solve.",
    "lens": ["Feasibility"],
    "level": "Product",
    "dimension": "Scope",
    "applies_when": ["ai:scoping", "capability:selecting"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-pattern-004",
    "source": "The AI Product Playbook",
    "type": "Pattern",
    "purpose": "Evaluate",
    "claim": "Deployment strategies for risk management: Shadow Mode, Canary Rollout, A/B Deployment. Never deploy directly to 100% of users.",
    "rationale": "Production environments differ from training environments. Staged rollout protects users and allows comparison against baselines.",
    "lens": ["Feasibility"],
    "level": "Product",
    "dimension": "Scope",
    "applies_when": ["deployment:planning", "risk:managing"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-signal-001",
    "source": "The AI Product Playbook",
    "type": "Signal",
    "purpose": "Detect",
    "claim": "High error rates in data entry or manual categorization signal opportunities for AI automation or augmentation.",
    "rationale": "AI excels at repetitive, data-rich tasks. Manual workarounds highlight prime candidates for automation.",
    "lens": ["Customer"],
    "level": "Product",
    "dimension": "Scope",
    "applies_when": ["opportunity:identifying", "friction:observed"],
    "strength": "Medium",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-signal-002",
    "source": "The AI Product Playbook",
    "type": "Signal",
    "purpose": "Detect",
    "claim": "Data Drift and Concept Drift degrade model performance. Set up dashboards tracking technical and business metrics with alert thresholds.",
    "rationale": "Models drift slowly. Proactive monitoring catches degradation before users are impacted.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["model:production", "monitoring:passive"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-example-001",
    "source": "The AI Product Playbook",
    "type": "Example",
    "purpose": "Illustrate",
    "claim": "Spam filter precision trade-off: prioritize high precision to avoid false positives, accepting some spam to avoid losing real mail.",
    "rationale": "The cost of false alarms exceeds the cost of missed spam, driving precision over recall.",
    "lens": ["Customer", "Business"],
    "level": "Product",
    "dimension": "Value",
    "outcome": "Worked",
    "context": ["B2C", "email"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-example-002",
    "source": "The AI Product Playbook",
    "type": "Example",
    "purpose": "Illustrate",
    "claim": "Cancer detection recall trade-off: prioritize high recall to avoid false negatives, accepting more false positives to catch all actual cases.",
    "rationale": "The cost of missing a true case vastly exceeds the cost of false alarms.",
    "lens": ["Customer", "Business"],
    "level": "Product",
    "dimension": "Value",
    "outcome": "Worked",
    "context": ["healthcare", "high-stakes"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ai-product-playbook-example-003",
    "source": "The AI Product Playbook",
    "type": "Example",
    "purpose": "Illustrate",
    "claim": "Gmail's Smart Compose capability template: text generation applied to domain problems by asking how generation solves your specific use case.",
    "rationale": "Successful AI features reveal reusable capabilities. Extract capability and apply to your context.",
    "lens": ["Business"],
    "level": "Product",
    "dimension": "Scope",
    "outcome": "Worked",
    "context": ["B2C", "productivity"],
    "strength": "Medium",
    "evidence_grade": "Primary"
  }
]

