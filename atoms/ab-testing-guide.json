[
  {
    "id": "ab-testing-heuristic-001",
    "source": "A/B Testing Guide",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "A/B testing fundamentally means making data-driven decisions in digital optimization. It removes guesswork, allowing decisions to be based on empirical data rather than assumptions. By testing one variable at a time, organizations can isolate the impact of specific changes.",
    "rationale": "This scientific rigor ensures improvements are directly attributable to the tested modification. Random assignment to groups minimizes bias and ensures observed differences can be attributed to the variation, not external factors.",
    "lens": ["Business", "Customer"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["decisions:assumption-based", "optimization:needed"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-heuristic-002",
    "source": "A/B Testing Guide",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "A strong hypothesis is the cornerstone of any successful A/B test. Use the 'If-Then-Because' structure: If [we implement this change], Then [we expect this outcome], Because [of this underlying reason or user psychology].",
    "rationale": "A well-formulated hypothesis transforms a vague idea into a testable statement, guiding the experiment's design and analysis. It articulates a clear prediction based on reasoned understanding of user behavior, not random guessing.",
    "lens": ["Customer"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["test:designing", "hypothesis:missing"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-heuristic-003",
    "source": "A/B Testing Guide",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "Define your Minimum Detectable Effect (MDE) before running tests. MDE is the smallest uplift you consider valuable. Smaller MDE requires larger sample size and longer duration. Setting MDE prevents wasted effort on trivial gains.",
    "rationale": "MDE directly influences required sample size and test duration. It clarifies what level of improvement makes the test 'worth it' from a business perspective and ensures tests are designed to detect meaningful changes.",
    "lens": ["Business", "Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["test:planning", "sample-size:calculating"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-heuristic-004",
    "source": "A/B Testing Guide",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "Run tests for at least one full business cycle (minimum 7 days) to account for daily and weekly variations in user behavior. Stopping earlier may capture skewed results from weekday/weekend differences or time-of-day patterns.",
    "rationale": "User behavior varies significantly based on day of week, time of day, and seasonal factors. A test stopped after only 3 days might not capture the full range of user behavior and could misrepresent true long-term performance.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["test:duration-setting", "cycles:business"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-heuristic-005",
    "source": "A/B Testing Guide",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "Use prioritization frameworks (PIE or ICE) to decide which experiments to run first. PIE = Potential + Importance + Ease. ICE = Impact × Confidence × Ease. Prioritize tests with higher scores to maximize returns.",
    "rationale": "Resources are limited. Systematic prioritization ensures experiments most likely to yield significant results with reasonable effort are run first, accelerating learning and optimization velocity.",
    "lens": ["Business", "Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["backlog:large", "prioritization:needed"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-heuristic-006",
    "source": "A/B Testing Guide",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "Define both primary and secondary metrics before launching any test. Primary metric determines the winner. Secondary metrics ensure the test doesn't negatively impact other important aspects of the business.",
    "rationale": "A test might win on primary metric but harm a crucial secondary metric, indicating a false positive or undesirable trade-off. Holistic evaluation prevents unintended consequences and provides comprehensive view of performance.",
    "lens": ["Business"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["metrics:defining", "test:designing"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-heuristic-007",
    "source": "A/B Testing Guide",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "Calculate sample size before starting tests. Required sample depends on: baseline conversion rate, MDE, statistical significance (typically 95%), and statistical power (typically 80%). Running with too few participants leads to false negatives.",
    "rationale": "Too few participants means observed differences are likely due to random chance, making it impossible to confidently declare a winning variation. Too many wastes resources. Proper calculation ensures reliable, actionable results.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["test:planning", "traffic:evaluating"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-heuristic-008",
    "source": "A/B Testing Guide",
    "type": "Heuristic",
    "purpose": "Evaluate",
    "claim": "Beyond statistical significance, calculate actual business impact and ROI. A statistically significant uplift might be too small to translate into meaningful business value, or implementation cost might outweigh gains.",
    "rationale": "Project the percentage improvement over specific periods. Factor in implementation costs (design, development, tool subscription). ROI = (Total Benefits – Total Costs) / Total Costs. Financial justification secures resources for future optimization.",
    "lens": ["Business"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["results:interpreting", "roi:calculating"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-failuremode-001",
    "source": "A/B Testing Guide",
    "type": "FailureMode",
    "purpose": "Warn",
    "claim": "Peeking at results before calculated sample size is reached dramatically increases false positives. Statistical significance typically stabilizes over time. Stopping prematurely means acting on noisy data, mistaking random fluctuations for genuine improvements.",
    "rationale": "A/B testing platforms show real-time updates, creating temptation to stop early. Initial swings in data are normal due to variance. Commit to predetermined sample size or duration and resist urge to check constantly.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["test:running", "impatience:present"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-failuremode-002",
    "source": "A/B Testing Guide",
    "type": "FailureMode",
    "purpose": "Warn",
    "claim": "Running multiple independent A/B tests on the same page simultaneously leads to confounded results. It becomes impossible to determine which change caused the observed effect, or if interaction between variations is skewing results.",
    "rationale": "Users might see combinations of changes that weren't intended for testing together. The impact of each individual element cannot be cleanly isolated. Prioritize tests and run sequentially, or use proper multivariate testing design.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["tests:overlapping", "page:shared"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-failuremode-003",
    "source": "A/B Testing Guide",
    "type": "FailureMode",
    "purpose": "Warn",
    "claim": "Changing multiple variables at once transforms A/B testing into something else without proper statistical design. You might know version B is better than A, but you won't know why. Test one variable at a time for pure A/B testing.",
    "rationale": "When multiple variables are changed, it's impossible to determine which specific change (or combination) caused the observed difference. Break complex ideas into smaller testable components. Use MVT only when specifically testing interactions.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["variables:multiple", "isolation:missing"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-failuremode-004",
    "source": "A/B Testing Guide",
    "type": "FailureMode",
    "purpose": "Warn",
    "claim": "Insufficient traffic for statistical significance leads to inconclusive results. Any observed differences are likely due to random chance, making it impossible to confidently declare a winner. This wastes effort and resources.",
    "rationale": "For low-traffic sites, test for larger MDE (only detect bigger uplifts), focus on high-traffic areas, extend test duration, or consider Bayesian methods. Always calculate required sample size before starting.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["traffic:low", "significance:unreachable"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-failuremode-005",
    "source": "A/B Testing Guide",
    "type": "FailureMode",
    "purpose": "Warn",
    "claim": "Ignoring secondary metrics can lead to unintended negative consequences. A test might show significant lift in primary conversion but simultaneously harm other important aspects—e.g., increasing clicks but also increasing customer service calls.",
    "rationale": "Define secondary metrics upfront. Monitor all relevant metrics throughout test. A 'winning' variation might need re-evaluation if it negatively impacts crucial secondary metrics. Balance short-term gains with long-term health.",
    "lens": ["Business"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["metrics:narrow-focus", "consequences:unintended"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-failuremode-006",
    "source": "A/B Testing Guide",
    "type": "FailureMode",
    "purpose": "Warn",
    "claim": "The novelty effect skews results when new variations initially attract higher engagement simply because they're new and different, not because they're inherently better. Users click out of curiosity, creating artificially inflated initial performance that diminishes over time.",
    "rationale": "Run tests longer than minimum statistical requirement (2-4 weeks). Monitor trend lines for stabilization rather than acting on initial spikes. Segment results to identify if novelty effect impacts only certain user groups.",
    "lens": ["Customer"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["variation:visually-striking", "results:early"],
    "strength": "Medium",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-pattern-001",
    "source": "A/B Testing Guide",
    "type": "Pattern",
    "purpose": "Evaluate",
    "claim": "A/B testing process: (1) Formulate hypothesis, (2) Create two versions identical except for one variable, (3) Randomly split traffic between versions, (4) Track user interactions on key metrics, (5) Analyze using statistical methods, (6) Validate or refute hypothesis.",
    "rationale": "This systematic approach ensures reliable and actionable insights. Random assignment minimizes bias. Single variable isolation enables attribution. Statistical analysis determines if differences are significant or noise.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["test:designing"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-pattern-002",
    "source": "A/B Testing Guide",
    "type": "Pattern",
    "purpose": "Evaluate",
    "claim": "PIE Framework: Potential (how much upside if test wins?) + Importance (how important is the page/element to business goals?) + Ease (how difficult to implement?). Score 1-10 each, sum for total priority score.",
    "rationale": "Prioritizing tests with higher PIE scores ensures resources are allocated to experiments most likely to yield significant results with reasonable effort. A small change on high-traffic critical page may have higher potential than big change on low-traffic page.",
    "lens": ["Business", "Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["prioritization:needed", "backlog:organizing"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-pattern-003",
    "source": "A/B Testing Guide",
    "type": "Pattern",
    "purpose": "Evaluate",
    "claim": "ICE Framework: Impact (magnitude of expected positive change) × Confidence (how confident this will produce expected outcome) × Ease (how easy to implement). Score 1-10 each. Emphasizes confidence as key factor.",
    "rationale": "Even high-impact, easy tests might not be worth pursuing if confidence in success is low. ICE encourages more thoughtful hypothesis generation by explicitly accounting for uncertainty in predictions.",
    "lens": ["Business", "Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["prioritization:needed", "confidence:variable"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-pattern-004",
    "source": "A/B Testing Guide",
    "type": "Pattern",
    "purpose": "Evaluate",
    "claim": "Test types by complexity: A/B (two versions) → A/B/n (multiple versions) → MVT (multiple variables simultaneously) → MAB (dynamic traffic allocation to winners). Each requires more traffic and statistical sophistication.",
    "rationale": "A/B tests one variable, providing clean isolation. A/B/n accelerates exploration but dilutes statistical power. MVT uncovers interactions but requires significantly more traffic. MAB optimizes during test but provides less certainty about differences.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["test-type:selecting", "complexity:evaluating"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-signal-001",
    "source": "A/B Testing Guide",
    "type": "Signal",
    "purpose": "Detect",
    "claim": "Statistical significance (p-value < 0.05) means less than 5% chance the observed difference occurred by random chance. 95% confidence level means if you ran the same experiment 100 times, 95 times you'd get similar results.",
    "rationale": "It is critical to reach statistical significance before declaring a winner. This reduces risk of implementing changes that appear to be winners but are actually just random fluctuation. Don't confuse statistical significance with business significance.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["results:interpreting", "significance:evaluating"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-signal-002",
    "source": "A/B Testing Guide",
    "type": "Signal",
    "purpose": "Detect",
    "claim": "Confidence intervals provide range within which true value likely falls. If confidence intervals for two versions do not overlap, it strongly suggests statistically significant difference. Overlapping intervals suggest difference might not be significant.",
    "rationale": "Confidence intervals provide more nuanced understanding than binary win/loss. They show not just whether one version is better, but how much better and the degree of uncertainty around that improvement.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["results:interpreting", "uncertainty:quantifying"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-signal-003",
    "source": "A/B Testing Guide",
    "type": "Signal",
    "purpose": "Detect",
    "claim": "Statistical power (typically 80%) is the probability of correctly detecting a real effect. Beta error (Type II) is failing to detect a real difference when one exists—concluding 'no winner' when variation was genuinely better.",
    "rationale": "Understanding power prevents false negatives and missed opportunities. To increase power, you need larger sample size or larger MDE. Tests must be adequately designed to detect the improvements you care about.",
    "lens": ["Feasibility"],
    "level": "Operating",
    "dimension": "Process",
    "applies_when": ["test:designing", "power:calculating"],
    "strength": "Medium",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-example-001",
    "source": "A/B Testing Guide",
    "type": "Example",
    "purpose": "Illustrate",
    "claim": "HubSpot headline test: Changed 'The World's Leading Marketing Platform' to 'How to Double Your Leads in 30 Days' → 40% increase in lead submissions. Demonstrates power of benefit-oriented messaging over generic positioning.",
    "rationale": "Users are more compelled by what a product can do for them than just what the product is. Small changes in copy can have massive impact on conversion. This simple test led to substantial business growth without major technological overhauls.",
    "lens": ["Customer", "Business"],
    "level": "Product",
    "dimension": "Value",
    "outcome": "Worked",
    "context": ["saas", "lead-generation", "landing-page"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-example-002",
    "source": "A/B Testing Guide",
    "type": "Example",
    "purpose": "Illustrate",
    "claim": "UK retailer checkout test: Made guest checkout option significantly more prominent → 14% increase in completed purchases. Demonstrates friction reduction directly impacts conversion.",
    "rationale": "Many users prefer quick guest checkout over creating an account for one-time purchases. Eliminating unnecessary steps or perceived barriers in checkout directly impacts conversion. Subtle changes to key flows can have big impact.",
    "lens": ["Customer"],
    "level": "Product",
    "dimension": "Value",
    "outcome": "Worked",
    "context": ["ecommerce", "checkout", "friction"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-example-003",
    "source": "A/B Testing Guide",
    "type": "Example",
    "purpose": "Illustrate",
    "claim": "Netflix thumbnail optimization: Continuously tests different images for content (character focus, expressions, moments) → 20-30% increase in content consumption for specific titles. Tests personalized thumbnails based on viewing history.",
    "rationale": "First impressions are critical—compelling visuals drive engagement. Even seemingly minor elements, when optimized at scale, have massive cumulative impact. Netflix runs constant tests, serving different thumbnails to different users.",
    "lens": ["Customer", "Business"],
    "level": "Product",
    "dimension": "Value",
    "outcome": "Worked",
    "context": ["streaming", "content-discovery", "personalization"],
    "strength": "High",
    "evidence_grade": "Primary"
  },
  {
    "id": "ab-testing-example-004",
    "source": "A/B Testing Guide",
    "type": "Example",
    "purpose": "Illustrate",
    "claim": "New York Times article recommendations: New algorithm balancing recency and topic relevance → 5% increase in average articles read per session. For millions of daily readers, this translates to significant increase in ad impressions and revenue.",
    "rationale": "Demonstrates how A/B testing drives subtle but impactful improvements in core engagement metrics for content businesses. Small percentage gains at scale create substantial long-term value in ad revenue and user loyalty.",
    "lens": ["Business"],
    "level": "Product",
    "dimension": "Value",
    "outcome": "Worked",
    "context": ["media", "publishing", "recommendations"],
    "strength": "High",
    "evidence_grade": "Primary"
  }
]

